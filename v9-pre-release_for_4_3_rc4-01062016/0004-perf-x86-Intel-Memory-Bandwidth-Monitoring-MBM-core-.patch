From 7dcd2fb95a8315bfbc2f794b6aa7948e200bf070 Mon Sep 17 00:00:00 2001
Message-Id: <7dcd2fb95a8315bfbc2f794b6aa7948e200bf070.1452107641.git.kanaka.d.juvva@linux.intel.com>
In-Reply-To: <cover.1452107641.git.kanaka.d.juvva@linux.intel.com>
References: <cover.1452107641.git.kanaka.d.juvva@linux.intel.com>
From: Kanaka Juvva <kanaka.d.juvva@linux.intel.com>
Date: Tue, 8 Dec 2015 05:13:44 -0800
Subject: [PATCH v9-pre-release_for_4_3_rc4-01062016 4/5]   perf,x86:  Intel
 Memory Bandwidth Monitoring (MBM) core functions

Intel Xeon CPU has MSRs that are serverd as Memory Bandwidth
Monitoring (MBM) event counters. MBM events' processing consists
of getting a free RMID, programming EVTSEL MSR with event type,
reading counters using RMID as a tag, and calculating bandwidth for
the current counter reading.

MSR reading is stored and is used as prevmsr value in bandwidth
calculation for the next MSR reading. struct sample abstracts out
current memory bandwidth reading and has the following elements:
struct sample {
u64 bytes;    MSR Reading
u64 bandwidth;  bandwidth calculated for the current
                MSR Reading
ktime_t prev_time; time stamp for the MSR Reading i.e. 'bytes and
                   bandwidth'
};

The following events types are implemented in Linux kernel and expose
two MBM counters via perf_event interface:

 - local_bw: local bandwidth bandwidth consumption of the process
                 , application or thread
 - local_avg_bw:  running average of the above
 - total_bw: local bandwidth consumption + bandwidth consumption
                 for remote memory accesses for the process,
                 application or thread
 - total_avg_bw: running average of the above i.e. total_bw

At present, MBM events are checked at one second interval provided
by the HRTIMER for the MBM event. Counters can overflow at most once in
a second and thus are read at least once in a second. Overflow is
detected and handled. Memory bandwidth is calculated for each bandwidth
type after reading a new value using the following formula.

Memory bandwidth = (current msr reading - previous msr reading)/
                   (current time - prev_time)$

Raw bandwidth is in bytes/sec. Kernel driver sets the .scale sysfs
attribute which is used by perf user space to get actual bandwidth
in uints as specified by '.unit' sysfs attribute. This driver sets
'.unit to MB/sec' and '.scale to upscalingfactor /1.0e6'.

Signed-off-by: Kanaka Juvva <kanaka.d.juvva@linux.intel.com>
---
 arch/x86/kernel/cpu/perf_event_intel_cqm.c | 482 ++++++++++++++++++++++++++++-
 1 file changed, 473 insertions(+), 9 deletions(-)

diff --git a/arch/x86/kernel/cpu/perf_event_intel_cqm.c b/arch/x86/kernel/cpu/perf_event_intel_cqm.c
index 2a656f1..812edf0 100644
--- a/arch/x86/kernel/cpu/perf_event_intel_cqm.c
+++ b/arch/x86/kernel/cpu/perf_event_intel_cqm.c
@@ -14,14 +14,49 @@
 #define MSR_IA32_QM_EVTSEL	0x0c8d
 
 /*
+ * MBM Counter is 24bits wide. MBM_CNTR_MAX defines max counter
+ * value
+ */
+#define MBM_CNTR_MAX		0xffffff
+
+/*
  * Expected time interval in ms between consecutive MSR reads for a given rmid
  */
 #define MBM_TIME_DELTA_EXP	1000
 
+/*
+ *  Minimum time interval in ms between consecutive MSR reads for a given rmid
+ */
+#define MBM_TIME_DELTA_MIN	100
+
+/*
+ * Number of milli secondss in a second
+ */
+#define MBM_CONVERSION_FACTOR	1000
+/*
+ * Minimum size for sliding window i.e. the minimum monitoring period for
+ * application(s). This fifo_size can be used for short duration monitoring
+ * since short duration monitoring will have less number of samples.
+ * Corresponding sliding window duration will be 10sec. mbm_window_size
+ * variable is used to set the current monitoring duration.
+ */
+#define MBM_FIFO_SIZE_MIN	10
+/*
+ * Maximum size for sliding window i.e. the maximum monitoring period that is
+ * supported. Corresponsing sliding window  duration for this fifo_size is
+ * 300sec. Typically long duration monitoring session can use this window size.
+ */
+#define MBM_FIFO_SIZE_MAX	300
+/*
+ * mbm_window_size is used to set current monitoring period. This means
+ * mbm_window_size defines the number of profiled samples to be stored in
+ * sliding window i.e. mbm_fifo.
+ */
+static u32 mbm_window_size = MBM_FIFO_SIZE_MIN;
 static u32 cqm_max_rmid = -1;
 static unsigned int cqm_l3_scale; /* supposedly cacheline size */
 static bool cqm_llc_occ, is_mbm;
-static int mbm_socket_max = 8;
+static u16  mbm_socket_max;
 
 /**
  * struct intel_pqr_state - State cache for the PQR MSR
@@ -77,8 +112,12 @@ struct mbm_pmu {
  */
 struct sample {
 	u64 bytes;
-	u64 bandwidth;
+	u64 runavg;
 	ktime_t prev_time;
+	u64 index;
+	u32 mbmfifo[MBM_FIFO_SIZE_MAX];
+	u32  fifoin;
+	u32  fifoout;
 };
 
 /*
@@ -91,6 +130,17 @@ static struct sample *mbm_total;
  */
 static struct sample *mbm_local;
 
+#define pkg_id	topology_physical_package_id(smp_processor_id())
+/*
+ * rmid_2_index returns the index for the rmid in mbm_local/mbm_total array.
+ * mbm_total[] and mbm_local[] are linearly indexed by core# * max number of
+ * rmids per socket, an example is given below
+ * RMID1 of Socket0:  vrmid  = 1
+ * RMID1 of Socket1:  vrmid =  1 * cqm_max_rmid + 1
+ * RMID1 of Socket2:  vrmid =  2 * cqm_max_rmid + 1
+ */
+#define rmid_2_index(rmid)  (pkg_id * cqm_max_rmid + rmid)
+
 static enum hrtimer_restart mbm_hrtimer_handle(struct hrtimer *hrtimer);
 
 /*
@@ -116,8 +166,19 @@ static cpumask_t cqm_cpumask;
 #define RMID_VAL_UNAVAIL	(1ULL << 62)
 
 #define QOS_L3_OCCUP_EVENT_ID	0x01
+/*
+ * MBM Event IDs as defined in SDM section 17.15.5
+ * Event IDs are used to program EVTSEL MSRs before reading mbm event counters
+ */
+enum mbm_evt_type {
+	QOS_MBM_TOTAL_EVENT_ID = 0x02,
+	QOS_MBM_LOCAL_EVENT_ID,
+	QOS_MBM_TOTAL_AVG_EVENT_ID,
+	QOS_MBM_LOCAL_AVG_EVENT_ID,
+};
 
-#define QOS_EVENT_MASK	QOS_L3_OCCUP_EVENT_ID
+#define QOS_MBM_AVG_EVENT_MASK 0x04
+#define QOS_MBM_LOCAL_EVENT_MASK 0x01
 
 /*
  * This is central to the rotation algorithm in __intel_cqm_rmid_rotate().
@@ -176,6 +237,7 @@ struct cqm_rmid_entry {
 	struct list_head list;
 	unsigned long queue_time;
 	bool is_cqm;
+	bool is_multi_event;
 };
 
 static void intel_cqm_free_rmid(struct cqm_rmid_entry *entry);
@@ -229,6 +291,29 @@ static inline struct cqm_rmid_entry *__rmid_entry(u32 rmid)
 	return entry;
 }
 
+/**
+ * mbm_reset_stats - reset stats for a given rmid for the current cpu
+ * @rmid:	rmid value
+ *
+ * vrmid: array index for mbm_total or mbm_local of the current core for the
+ * given rmid
+ *
+ * mbs_total[] and mbm_local[] are linearly indexed by core number * max number
+ * rmids per socket, an example is given below
+ * RMID1 of Socket0:  vrmid  = 1
+ * RMID1 of Socket1:  vrmid =  1 * CQM_MAX_RMID + 1
+ * RMID1 of Socket2:  vrmid =  2 * CQM_MAX_RMID + 1
+ */
+static void mbm_reset_stats(u32 rmid)
+{
+	u32  vrmid =  rmid_2_index(rmid);
+
+	if (!is_mbm)
+		return;
+	memset(&mbm_local[vrmid], 0, sizeof(struct sample));
+	memset(&mbm_total[vrmid], 0, sizeof(struct sample));
+}
+
 /*
  * Returns < 0 on fail.
  *
@@ -260,6 +345,7 @@ static void __put_rmid(u32 rmid)
 
 	entry->queue_time = jiffies;
 	entry->state = RMID_YOUNG;
+	mbm_reset_stats(rmid);
 
 	/*
 	 * If the RMID is used for measuring LLC_OCCUPANCY, put it in
@@ -345,8 +431,15 @@ static bool __match_event(struct perf_event *a, struct perf_event *b)
 	/*
 	 * Events that target same task are placed into the same cache group.
 	 */
-	if (a->hw.target == b->hw.target)
+	if (a->hw.target == b->hw.target) {
+		if (a->attr.config  != b->attr.config) {
+			struct cqm_rmid_entry *entry;
+
+				entry = __rmid_entry(a->hw.cqm_rmid);
+				entry->is_multi_event = true;
+		}
 		return true;
+	}
 
 	/*
 	 * Are we an inherited event?
@@ -456,6 +549,7 @@ static bool __conflict_event(struct perf_event *a, struct perf_event *b)
 struct rmid_read {
 	u32 rmid;
 	atomic64_t value;
+	enum mbm_evt_type evt_type;
 };
 
 static void __intel_cqm_event_count(void *info);
@@ -583,6 +677,228 @@ static void intel_cqm_free_rmid(struct cqm_rmid_entry *entry)
 }
 
 /*
+ * Slide the window by 1 and calculate the sum of the last
+ * mbm_window_size-1  bandwidth  values.
+ * fifoout is the current position of the window.
+ * Increment the fifoout by 1 to slide the window by 1.
+ *
+ * Calcalute the bandwidth from ++fifiout  to ( ++fifoout + mbm_window_size -1)
+ * e.g.fifoout =1;   Bandwidth1 Bandwidth2 ..... Bandwidthn are the
+ * sliding window values where n is size of the sliding window
+ * bandwidth sum:  val  =  Bandwidth2 + Bandwidth3 + .. Bandwidthn
+ */
+
+static u32 __mbm_fifo_sum_lastn_out(struct sample *bw_stat)
+{
+	u32 val = 0, i, j, index;
+
+	if (++bw_stat->fifoout >=  mbm_window_size)
+		bw_stat->fifoout =  0;
+	index =  bw_stat->fifoout;
+	for (i = 0; i < mbm_window_size - 1; i++) {
+		if (index + i == mbm_window_size)
+			j = index + i - mbm_window_size;
+		else
+			j = index + i;
+		val += bw_stat->mbmfifo[j];
+	}
+	return val;
+}
+
+/*
+ * store current sample's bw value in sliding window at the
+ * location fifoin. Increment fifoin. Check if fifoin has reached
+ * max_window_size. If yes reset it to beginning i.e. zero
+ *
+ */
+static void mbm_fifo_in(struct sample *bw_stat, u32 val)
+{
+	bw_stat->mbmfifo[bw_stat->fifoin] = val;
+	if (++bw_stat->fifoin == mbm_window_size)
+		bw_stat->fifoin = 0;
+}
+
+/*
+ * rmid_read_mbm checks whether it is LOCAL or Total MBM event and reads
+ * its MSR counter. Check whether overflow occurred and handle it. Calculate
+ * current bandwidth and updates its running average.
+ *
+ * MBM Counter Overflow:
+ * Calculation of Current Bandwidth value:
+ * If MSR is read within last 100ms, then we rturn the previous value
+ * Currently perf receommends keeping 100ms between samples. Driver uses
+ * this guideline. If the MSR was Read with in last 100ms, why  incur an
+ * extra overhead of doing the MSR reads again.
+ *
+ * Bandwidth is calculated as:
+ * memory bandwidth = (difference of  two msr counter values )/time difference
+ *
+ * cum_avg = Running Average of bandwidth with last 'n' bandwidth values of
+ * the samples that are processed
+ *
+ * Sliding window is used to save the last 'n' samples. Where,
+ * n = sliding_window_size and results in sliding window duration of 'n' secs.
+ * The sliding window size by default set to
+ * MBM_FIFO_SIZE_MIN. User can configure it to the values in the range
+ * (MBM_FIFO_SIZE_MIN,MBM_FIFO_SIZE_MAX). The range for sliding window
+ * is chosen based on a general criteria for monitoring duration. Example
+ * for a short lived application, 10sec monitoring period gives
+ * good characterization of its bandwidth consumption. For an application
+ * that runs for longer duration, 300sec monitoring period gives better
+ * characterization of its bandwidth consumption. Since the running average
+ * calculated for total monitoring period, user gets the most accurate
+ * average bandwidth for each monitoring period.
+ *
+ * Conversion from Bytes/sec to MB/sec:
+ * current sample's  bandwidth is calculated in Bytes/sec.
+ * Perf user space gets the values in units as specified by .scale and .unit
+ * atrributes for the MBM event.
+ */
+static u64 rmid_read_mbm(unsigned int rmid, enum mbm_evt_type evt_type)
+{
+	u64  val, currentmsr, diff_time,  currentbw, bytes, prevavg;
+	bool overflow = false, first = false;
+	ktime_t cur_time;
+	u32 eventid, index;
+	struct sample *mbm_current;
+	u32 vrmid = rmid_2_index(rmid);
+
+	cur_time = ktime_get();
+	if (evt_type & QOS_MBM_LOCAL_EVENT_MASK) {
+		mbm_current = &mbm_local[vrmid];
+		eventid     =  QOS_MBM_LOCAL_EVENT_ID;
+	} else {
+		mbm_current = &mbm_total[vrmid];
+		eventid     = QOS_MBM_TOTAL_EVENT_ID;
+	}
+
+	prevavg = mbm_current->runavg;
+	currentbw = mbm_current->mbmfifo[mbm_current->fifoin];
+	diff_time = ktime_ms_delta(cur_time,
+				   mbm_current->prev_time);
+	if (diff_time > MBM_TIME_DELTA_MIN) {
+
+		wrmsr(MSR_IA32_QM_EVTSEL, eventid, rmid);
+		rdmsrl(MSR_IA32_QM_CTR, val);
+
+		if (val & (RMID_VAL_ERROR | RMID_VAL_UNAVAIL))
+			return val;
+
+		bytes = mbm_current->bytes;
+		currentmsr = val;
+		val &= MBM_CNTR_MAX;
+		/* if MSR current read value is less than MSR previous read
+		 * value then it is an overflow. MSR values are increasing
+		 * when bandwidth consumption for the thread is non-zero;
+		 * Overflow occurs, When MBM counter value reaches its
+		 * maximum i.e. MBM_CNTR_MAX.
+		 *
+		 * After overflow, MSR current value goes back to zero and
+		 * starts increasing again at the rate of bandwidth.
+		 *
+		 * Overflow handling:
+		 * First overflow is detected by comparing current msr values
+		 * will with the last read value. If current msr value is less
+		 * than previous value then it is an overflow. When overflow
+		 * occurs, (MBM_CONTR_MAX - prev msr value) is added the current
+		 * msr value to the get actual value.
+		 */
+
+		if (val < bytes) {
+			val = MBM_CNTR_MAX - bytes + val + 1;
+			overflow = true;
+		} else
+			val = val - bytes;
+
+		/*
+		 * MBM_TIME_DELTA_EXP is picked as per MBM specs. As per
+		 * hardware functionality, overflow can occur maximum once in a
+		 * second. So latest we want to read the MSR counters is 1000ms.
+		 * Minimum time interval between two MSR reads is 100ms. If
+		 * read_rmid_mbm is called with in less than 100ms, use the
+		 * previous sammple since perf also recommends to use the
+		 * minimum sampling period of 100ms.
+		 */
+
+		if ((diff_time > MBM_TIME_DELTA_EXP) && (!prevavg))
+		/* First sample, we can't use the time delta */
+			first = true;
+
+		if ((diff_time <= (MBM_TIME_DELTA_EXP + MBM_TIME_DELTA_MIN))  ||
+			   overflow || first) {
+			int averagebw, bwsum;
+
+			/*
+			 * For the first 'mbm_window_size -1' samples
+			 * calculate average by adding the current sample's
+			 * bandwidth to the sum of existing bandwidth values and
+			 * dividing the sum with the #samples profiled so far
+			*/
+			averagebw = 0;
+			index = mbm_current->index;
+			currentbw =  (val * MSEC_PER_SEC) / diff_time;
+			averagebw = currentbw;
+			if (index    && (index < mbm_window_size)) {
+				averagebw = prevavg  + currentbw / index -
+				    prevavg / index;
+			} else  if (index >= mbm_window_size) {
+				/*
+				 * Compute the sum of bandwidth for recent n-1
+				 * sampland slide the window by 1
+				 */
+				bwsum = __mbm_fifo_sum_lastn_out(mbm_current);
+				/*
+				 * recalculate the running average by adding
+				 * current bandwidth  and
+				 * __mbm_fifo_sum_lastn_out which is the sum of
+				 * last bandwidth values from the sliding
+				 * window. The sum divided by mbm_window_size'
+				 * is the new running average of the MBM
+				 * Bandwidth
+				 */
+				averagebw = (bwsum + currentbw) /
+					     mbm_window_size;
+			}
+
+			/* save the current sample's bandwidth in fifo */
+			mbm_fifo_in(mbm_current, currentbw);
+			mbm_current->index++;
+			mbm_current->runavg = averagebw;
+			mbm_current->bytes = currentmsr;
+			mbm_current->prev_time = cur_time;
+
+		}
+	}
+	/* No change, return the existing running average */
+	if (evt_type & QOS_MBM_AVG_EVENT_MASK)
+		return mbm_current->runavg;
+	else
+		return currentbw;
+}
+
+static void intel_mbm_event_update(struct perf_event *event)
+{
+	unsigned int rmid;
+	u64 val = 0;
+
+	/*
+	 * Task events are handled by intel_cqm_event_count().
+	 */
+
+	rmid = event->hw.cqm_rmid;
+	if (!__rmid_valid(rmid))
+		return;
+	val = rmid_read_mbm(rmid, event->attr.config);
+	/*
+	 * Ignore this reading on error states and do not update the value.
+	 */
+	if (val & (RMID_VAL_ERROR | RMID_VAL_UNAVAIL))
+		return;
+
+	local64_set(&event->count, val);
+}
+
+/*
  * Initially use this constant for both the limbo queue time and the
  * rotation timer interval, pmu::hrtimer_interval_ms.
  *
@@ -961,6 +1277,13 @@ static void intel_cqm_event_read(struct perf_event *event)
 	if (event->cpu == -1)
 		return;
 
+	if  ((event->attr.config >= QOS_MBM_TOTAL_EVENT_ID) &&
+	     (event->attr.config <= QOS_MBM_LOCAL_EVENT_ID))
+		intel_mbm_event_update(event);
+
+	if (event->attr.config !=  QOS_L3_OCCUP_EVENT_ID)
+		return;
+
 	raw_spin_lock_irqsave(&cache_lock, flags);
 	rmid = event->hw.cqm_rmid;
 
@@ -998,6 +1321,38 @@ static inline bool cqm_group_leader(struct perf_event *event)
 	return !list_empty(&event->hw.cqm_groups_entry);
 }
 
+static void mbm_stop_hrtimer(struct mbm_pmu *pmu)
+{
+	hrtimer_cancel(&pmu->hrtimer);
+}
+
+static void __intel_mbm_event_count(void *info)
+{
+	struct rmid_read *rr = info;
+	u64 val;
+
+	val = rmid_read_mbm(rr->rmid, rr->evt_type);
+	if (val & (RMID_VAL_ERROR | RMID_VAL_UNAVAIL))
+		return;
+	atomic64_add(val, &rr->value);
+}
+
+static u64 intel_mbm_event_count(struct perf_event *event, struct rmid_read *rr)
+{
+	struct mbm_pmu *pmu = __this_cpu_read(mbm_pmu);
+
+	on_each_cpu_mask(&cqm_cpumask, __intel_mbm_event_count,
+		   rr, 1);
+	if (pmu) {
+		pmu->n_active--;
+		if (pmu->n_active == 0)
+			mbm_stop_hrtimer(pmu);
+	}
+	if (event->hw.cqm_rmid == rr->rmid)
+		local64_set(&event->count, atomic64_read(&rr->value));
+	return __perf_event_count(event);
+
+}
 static u64 intel_cqm_event_count(struct perf_event *event)
 {
 	unsigned long flags;
@@ -1022,8 +1377,13 @@ static u64 intel_cqm_event_count(struct perf_event *event)
 	 * specific packages - we forfeit that ability when we create
 	 * task events.
 	 */
-	if (!cqm_group_leader(event))
-		return 0;
+	if (!cqm_group_leader(event)) {
+		struct cqm_rmid_entry *entry;
+
+		entry = __rmid_entry(event->hw.cqm_rmid);
+		if (!entry->is_multi_event)
+			return 0;
+	}
 
 	/*
 	 * Getting up-to-date values requires an SMP IPI which is not
@@ -1049,8 +1409,14 @@ static u64 intel_cqm_event_count(struct perf_event *event)
 	if (!__rmid_valid(rr.rmid))
 		goto out;
 
-	on_each_cpu_mask(&cqm_cpumask, __intel_cqm_event_count, &rr, 1);
+	if (event->attr.config == QOS_L3_OCCUP_EVENT_ID)
+		on_each_cpu_mask(&cqm_cpumask, __intel_cqm_event_count, &rr, 1);
 
+	if (((event->attr.config >= QOS_MBM_TOTAL_EVENT_ID) &&
+	     (event->attr.config <= QOS_MBM_LOCAL_AVG_EVENT_ID))  && (is_mbm)) {
+		rr.evt_type = event->attr.config;
+		return intel_mbm_event_count(event, &rr);
+	}
 	raw_spin_lock_irqsave(&cache_lock, flags);
 	if (event->hw.cqm_rmid == rr.rmid)
 		local64_set(&event->count, atomic64_read(&rr.value));
@@ -1059,12 +1425,23 @@ out:
 	return __perf_event_count(event);
 }
 
+static void mbm_start_hrtimer(struct mbm_pmu *pmu)
+{
+	hrtimer_start_range_ns(&(pmu->hrtimer),
+				 pmu->timer_interval, 0,
+				 HRTIMER_MODE_REL_PINNED);
+}
+
 static enum hrtimer_restart mbm_hrtimer_handle(struct hrtimer *hrtimer)
 {
 	struct mbm_pmu *pmu = __this_cpu_read(mbm_pmu);
+	struct perf_event *event;
 
 	if (!pmu->n_active)
 		return HRTIMER_NORESTART;
+	list_for_each_entry(event, &pmu->active_list, active_entry)
+		intel_mbm_event_update(event);
+	hrtimer_forward_now(hrtimer, pmu->timer_interval);
 	return HRTIMER_RESTART;
 }
 
@@ -1076,6 +1453,24 @@ static void mbm_hrtimer_init(struct mbm_pmu *pmu)
 	hr->function = mbm_hrtimer_handle;
 }
 
+static void intel_mbm_event_start(struct perf_event *event, int mode)
+{
+
+	if (((event->attr.config >= QOS_MBM_TOTAL_EVENT_ID) &&
+	     (event->attr.config <= QOS_MBM_LOCAL_EVENT_ID))  && (is_mbm)) {
+		struct mbm_pmu *pmu = __this_cpu_read(mbm_pmu);
+
+		if (pmu) {
+			pmu->n_active++;
+			list_add_tail(&event->active_entry,
+				      &pmu->active_list);
+			if (pmu->n_active == 1)
+				mbm_start_hrtimer(pmu);
+		}
+	}
+
+}
+
 static void intel_cqm_event_start(struct perf_event *event, int mode)
 {
 	struct intel_pqr_state *state = this_cpu_ptr(&pqr_state);
@@ -1095,6 +1490,24 @@ static void intel_cqm_event_start(struct perf_event *event, int mode)
 
 	state->rmid = rmid;
 	wrmsr(MSR_IA32_PQR_ASSOC, rmid, state->closid);
+	intel_mbm_event_start(event, mode);
+
+}
+
+static void intel_mbm_event_stop(struct perf_event *event, int mode)
+{
+	struct mbm_pmu *pmu = __this_cpu_read(mbm_pmu);
+
+	if (pmu) {
+		intel_mbm_event_update(event);
+		if ((pmu->n_active >  0) && (event->cpu != -1))
+			pmu->n_active--;
+			if (pmu->n_active == 0)
+				mbm_stop_hrtimer(pmu);
+		if (!list_empty(&event->active_entry))
+			list_del(&event->active_entry);
+	}
+
 }
 
 static void intel_cqm_event_stop(struct perf_event *event, int mode)
@@ -1106,7 +1519,12 @@ static void intel_cqm_event_stop(struct perf_event *event, int mode)
 
 	event->hw.cqm_state |= PERF_HES_STOPPED;
 
-	intel_cqm_event_read(event);
+	if (event->attr.config == QOS_L3_OCCUP_EVENT_ID)
+		intel_cqm_event_read(event);
+
+	if ((event->attr.config >= QOS_MBM_TOTAL_EVENT_ID) &&
+	    (event->attr.config <= QOS_MBM_LOCAL_EVENT_ID))
+		intel_mbm_event_update(event);
 
 	if (!--state->rmid_usecnt) {
 		state->rmid = 0;
@@ -1114,6 +1532,8 @@ static void intel_cqm_event_stop(struct perf_event *event, int mode)
 	} else {
 		WARN_ON_ONCE(!state->rmid);
 	}
+
+	intel_mbm_event_stop(event, mode);
 }
 
 static int intel_cqm_event_add(struct perf_event *event, int mode)
@@ -1181,7 +1601,8 @@ static int intel_cqm_event_init(struct perf_event *event)
 	if (event->attr.type != intel_cqm_pmu.type)
 		return -ENOENT;
 
-	if (event->attr.config & ~QOS_EVENT_MASK)
+	if ((event->attr.config < QOS_L3_OCCUP_EVENT_ID) ||
+	     (event->attr.config > QOS_MBM_LOCAL_AVG_EVENT_ID))
 		return -EINVAL;
 
 	/* unsupported modes and filters */
@@ -1352,6 +1773,16 @@ max_recycle_threshold_show(struct device *dev, struct device_attribute *attr,
 }
 
 static ssize_t
+sliding_window_size_show(struct device *dev, struct device_attribute *attr,
+		char *page)
+{
+	ssize_t rv;
+
+	rv = snprintf(page, PAGE_SIZE-1, "%u\n", mbm_window_size);
+	return rv;
+}
+
+static ssize_t
 max_recycle_threshold_store(struct device *dev,
 			    struct device_attribute *attr,
 			    const char *buf, size_t count)
@@ -1379,10 +1810,36 @@ max_recycle_threshold_store(struct device *dev,
 	return count;
 }
 
+static ssize_t
+sliding_window_size_store(struct device *dev,
+			  struct device_attribute *attr,
+			  const char *buf, size_t count)
+{
+	unsigned int bytes;
+	int ret;
+
+	ret = kstrtouint(buf, 0, &bytes);
+	if (ret)
+		return ret;
+
+	mutex_lock(&cache_mutex);
+	if (bytes >= MBM_FIFO_SIZE_MIN && bytes <= MBM_FIFO_SIZE_MAX)
+		mbm_window_size = bytes;
+	else {
+		mutex_unlock(&cache_mutex);
+		return -EINVAL;
+	}
+	mutex_unlock(&cache_mutex);
+
+	return count;
+}
+
 static DEVICE_ATTR_RW(max_recycle_threshold);
+static DEVICE_ATTR_RW(sliding_window_size);
 
 static struct attribute *intel_cqm_attrs[] = {
 	&dev_attr_max_recycle_threshold.attr,
+	&dev_attr_sliding_window_size.attr,
 	NULL,
 };
 
@@ -1459,6 +1916,7 @@ static void intel_cqm_cpu_exit(unsigned int cpu)
 {
 	int phys_id = topology_physical_package_id(cpu);
 	int i;
+	struct mbm_pmu *pmu = per_cpu(mbm_pmu, cpu);
 
 	/*
 	 * Is @cpu a designated cqm reader?
@@ -1475,6 +1933,11 @@ static void intel_cqm_cpu_exit(unsigned int cpu)
 			break;
 		}
 	}
+
+	/* cancel overflow polling timer for CPU */
+	if (pmu)
+		mbm_stop_hrtimer(pmu);
+
 }
 
 static int intel_cqm_cpu_notifier(struct notifier_block *nb,
@@ -1539,6 +2002,7 @@ static int  intel_mbm_init(void)
 		mbm_socket_max = max(mbm_socket_max,
 				     topology_physical_package_id(i));
 	}
+	mbm_socket_max++;
 
 	array_size = (cqm_max_rmid + 1) * mbm_socket_max;
 	mbm_local = kzalloc_node(sizeof(struct sample) * array_size,
-- 
2.1.0

